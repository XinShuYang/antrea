[
  {
   "id": "identity_1",
   "conversations": [
    {
     "from": "human",
     "value": "title: Egress should not be applied to traffic destined for ServiceCIDRs. body: **Describe the bug**\r\n\r\nThe issue was reported by @sroot1986, @claknee, @robbo10:\r\n\r\nWhen AntreaProxy is asked to skip some Services by configuring `skipServices` or is not running at all, the traffic destined for Service ClusterIP is supposed to reach host network and be proxied by kube-proxy, or be processed by the host directly (e.g. [NodeLocal DNSCache](https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/)). However, if Egress is applied to the Pod, the traffic would be forwarded to Egress Node and would be proxied remotely, as opposed to locally, which could incur  performance issue and unexpected behaviors.\r\n\r\n\r\n**To Reproduce**\r\n\r\n\r\n1. Install [NodeLocal DNSCache](https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/)\r\n2. Configure AntreaProxy to skip DNS service, following [the doc](https://github.com/antrea-io/antrea/blob/main/docs/antrea-proxy.md#when-you-are-using-nodelocal-dnscache)\r\n3. Apply Egress to a Pod, which is scheduled to a Node different from the Egress Node.\r\n4. Trigger DNS query from the Pod, the Node local DNS server won't receive the request and the resolution may fail\r\n\r\n**Expected**\r\n\r\n\r\nPod-to-Service ClusterIP should not be forwarded to Egress Node and should be handled locally regardless of how AntreaProxy is configured.\r\n\r\n**Versions:**\r\n\r\n - Antrea version (Docker image tag). v1.11.3, v1.12.2, v1.13.1\r\n\r\n"
    },
    {
     "from": "gpt",
     "value": "['transit']"
    }
   ]
  },
  {
   "id": "identity_3",
   "conversations": [
    {
     "from": "human",
     "value": "title: antrea-gw0 doesn't respond IPv6 neighbor solicitation message with a link-local source address in ipv6 env. body: **Describe the bug**\r\n\r\n\r\nOn an IPv6 cluster (suitable for both IPv6 only and dual-stack cluster), we observe that Pod can not access addresses on a different Node or on external world after the agent run for some time.\r\n\r\nAfter some investigations inside the client Pod, we found that the IPv6 neighbor referring to antrea-gw0 is with state \"FAILED\". With packet capturing on both Pod interface and antrea-gw0, we observed that Pod's vNIC is trying to use its link-local address to send Neighbor Solicitation message, and antrea-gw0 has received it but not respondes it. Then we observed that a route with the antrea-gw0's link-local address CIDR (`fe80::/64 dev antrea-gw0`) is lost on the Node. It is suspected that Linux kernel has dropped the Pod's NS packets because of the missing routes ( similar to rp_filter check), so I made some test to manually add the route back, then the traffic issue is resolved.\r\n\r\nThen we found this log in antrea-agent,\r\n```\r\n Deleting unknown route {Ifindex: 5 Dst: fe80::/64 Src:  Gw:  Flags: [] Table: 254 Realm: 0}\r\n```\r\n\r\nObviously, it is a bug in antrea-agent to delete such routes.\r\n\r\n**To Reproduce**\r\n\r\n\r\nDeploy IPv6 only or dual-stack cluster, try to access an external address from a Pod, e.g., nslookup google.com . After several trials, the access would be failed. Then check the IPv6 neighbors inside Pod, it would show the entry to antrea-gw0's global IPv6 address is with state \"FAILED\"\r\n\r\n**Expected**\r\n\r\n\r\nPod should always successfully access an external address in IPv6 cluster.\r\n\r\n**Actual behavior**\r\n\r\n\r\nThe access is failed after some trials.\r\n\r\n**Versions:**\r\n\r\nAntrea: latest\r\n\r\n**Additional context**\r\n\r\n\r\n\r\n"
    },
    {
     "from": "gpt",
     "value": "['transit']"
    }
   ]
  },
  {
   "id": "identity_4",
   "conversations": [
    {
     "from": "human",
     "value": "title: Figure out a stable path for SSL installation in install-ovs.ps1. body: **Describe the bug**\r\n\r\nHi Antrea ! \r\n\r\nSo, our SSL story https://github.com/antrea-io/antrea/blob/main/hack/windows/Install-OVS.ps1#L217 Appears to give some folks problems, but not other folks... I suppose its based on where they're pulling things down from and wether they trust sites like `fulgan.com`.\r\n\r\nThe hardcoded **fulgan.com** URL is likely the root cause of all this - we need to find a way to more gracefully and intentionally configure SSL and heavily document it so there is no ambiguity or magic behavior in the future.  \r\n\r\nThis has hit us in the past (https://github.com/kubernetes-sigs/sig-windows-dev-tools/issues/198) and there was some work done to make it better, but I don't fully understand what the end result should be...   the TLDR is that, \r\n\r\n## requirements\r\n\r\n- we just need an intentional, configurable SSL and OVS installation process for windows \r\n- that is easy for people to understand and/or troubleshoot, \r\n- and which may use URLs from the internet for defaults, but all the while allows for alt. URLs in case those are unavailable.\r\n\r\n\r\n**To Reproduce**\r\n\r\nNot quite sure how to reproduce this, but we've seen it from time to time in the past \r\n\r\n**Expected**\r\n\r\n\r\n**Actual behavior** \r\n\r\nIts not clear what to do in cases where  \r\n\r\n```\r\nDownload SSL files failed, URL: https://indy.fulgan.com/SSL/openssl-1.0.2u-x64_86-win64.zip\r\n```\r\nhappens.  We need a workaround or fallback url for people who cannot access, or do not trust, this HTTPS endpoint, that can be plumbed in or just automatically works in the install-ovs.ps1 scripts\r\n "
    },
    {
     "from": "gpt",
     "value": "['windows']"
    }
   ]
  },
  {
   "id": "identity_5",
   "conversations": [
    {
     "from": "human",
     "value": "title: No distribution from PODs to IPv6 services with kernel linux-6.5. body: ## Description\r\n\r\nWhen using kernel `linux-6.5` all connects from within a POD to an IPv6 service ends up in just one endpoint (POD).\r\n\r\n_Only IPv6 from within a POD fails_:\r\n* IPv4 traffic from a POD to a service is distributed ok (works)\r\n* Both IPv4 and IPv6 traffic from main netns on a node, or from an external client is distributed ok (works)\r\n* Everything works fine on `linux-6.4.12`\r\n\r\n## To Reproduce\r\n\r\n* Start a K8s cluster on machines using kernel `linux-6.5`\r\n* Create a deployment and an IPv6 service. My manifest for test below:\r\n\r\nantrea-test.yaml\r\n\r\n```yaml\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: tserver-mconnect\r\nspec:\r\n  replicas: 4\r\n  selector:\r\n    matchLabels:\r\n      app: tserver-mconnect\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: tserver-mconnect\r\n    spec:\r\n      containers:\r\n      - name: tserver-mconnect\r\n        image: docker.io/uablrek/tserver:latest\r\n        imagePullPolicy: IfNotPresent\r\n#        securityContext:\r\n#          privileged: true\r\n        env:\r\n        - name: SERVERS\r\n          value: \"mconnect\"\r\n        ports:\r\n        - name: mconnect\r\n          containerPort: 5001\r\n---\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: tserver-mconnect\r\nspec:\r\n  ipFamilyPolicy: SingleStack\r\n  ipFamilies:\r\n  - IPv6\r\n  selector:\r\n    app: tserver-mconnect\r\n  type: LoadBalancer\r\n#  allocateLoadBalancerNodePorts: false\r\n  ports:\r\n  - port: 5001\r\n    name: mconnect\r\n```\r\n\r\n\r\n\r\n\r\n* Make a number of connects to the IPv6 service from within a POD. I am using [mconnect](https://github.com/Nordix/mconnect)\r\n\r\n## Expected\r\n\r\n```\r\nuname -a\r\nLinux vm-002 6.4.12 #2 SMP PREEMPT_DYNAMIC Thu Aug 24 06:52:27 CEST 2023 x86_64 GNU/Linux\r\npod=$(kubectl get pods -l app=tserver-mconnect -o name | head -1)\r\nkubectl exec $pod -- mconnect -address tserver-mconnect:5001 --nconn 100\r\nFailed connects; 0\r\nFailed reads; 0\r\ntserver-mconnect-85fb6588d6-r4hw2 22\r\ntserver-mconnect-85fb6588d6-tqvm7 26\r\ntserver-mconnect-85fb6588d6-r9bhw 23\r\ntserver-mconnect-85fb6588d6-gjk2l 29\r\n```\r\nTraffic is about evenly distributed among the (4) server PODs\r\n\r\n## Actual behavior\r\n\r\n```\r\nuname -a\r\nLinux vm-002 6.5.0 #4 SMP PREEMPT_DYNAMIC Wed Aug 30 08:04:17 CEST 2023 x86_64 GNU/Linux\r\npod=$(kubectl get pods -l app=tserver-mconnect -o name | head -1)\r\nkubectl exec $pod -- mconnect -address tserver-mconnect:5001 --nconn 100\r\nFailed connects; 0\r\nFailed reads; 0\r\ntserver-mconnect-85fb6588d6-2t2v7 100\r\n```\r\nHere all 100 connects goes to just one endpoint\r\n\r\n## Versions:\r\n\r\n```\r\n  14:17:42 Server Version: v1.28.1\r\n  14:17:42 CNI-plugin; antrea-ubuntu:v1.13.0\"\r\n  14:17:42 Proxy-mode: \"ipvs\"\r\n  14:17:42 crio version 1.27.1\r\n```\r\n\r\n## Additional context\r\n\r\nI have tried with `proxy-mode=iptables` and with `Containerd [v1.7.0]` but the result is the same. I seems to be kernel version dependent only.\r\n\r\nI am using kvm VM and the `linux-6.5` kernel is built with about the same config as my `linux-6.4.12` that works. I can provide the configs if you want.\r\n\r\nI have installed Antrea using an un-modified https://github.com/antrea-io/antrea/releases/download/v1.13.0/antrea.yml\r\n\r\nI, or a colleague of mine, can pin-point the kernel commit with `git bisect`, but that is a fair amount of work so we will let you take a look first.\r\n\r\nAslo, _this is not urgent for us_. At this point it's more an observation."
    },
    {
     "from": "gpt",
     "value": "['proxy', 'transit']"
    }
   ]
  },
  {
   "id": "identity_8",
   "conversations": [
    {
     "from": "human",
     "value": "title: antrea \"Clustering\"   is not  OS aware . body: **Describe the bug**\r\n\r\nfound some ominous logs today with @aroradaman while debugging a mixed OS linux/windows cluster.\r\n\r\n```\r\n\r\nI0823 11:49:58.077143       1 egress_controller.go:819] Stopped watch for EgressGroup, total items received: 0\r\nI0823 11:49:58.077172       1 egress_controller.go:795] Starting watch for EgressGroup\r\nI0823 11:49:58.081636       1 egress_controller.go:816] Started watch for EgressGroup\r\nI0823 11:49:58.081795       1 egress_controller.go:844] Received 0 init events for EgressGroup\r\nI0823 11:50:15.410463       1 networkpolicy_controller.go:778] Stopped watch for AppliedToGroup, total items received: 0\r\nI0823 11:50:15.410493       1 networkpolicy_controller.go:761] Starting watch for AppliedToGroup\r\nI0823 11:50:15.414468       1 networkpolicy_controller.go:774] Started watch for AppliedToGroup\r\nI0823 11:50:15.414531       1 networkpolicy_controller.go:804] Received 0 init events for AppliedToGroup\r\nE0823 11:50:37.547900       1 cluster.go:379] \"Failed to rejoin any members\" err= members=[10.221.159.239 10.221.159.223]\r\nE0823 11:51:37.547873       1 cluster.go:379] \"Failed to rejoin any members\" err=<\r\n        2 errors occurred:\r\n                * Failed to join 10.221.159.239:10351: dial tcp 10.221.159.239:10351: connect: connection refused\r\n                * Failed to join 10.221.159.223:10351: dial tcp 10.221.159.223:10351: connect: connection refused\r\n                *\r\n ```\r\n\r\n**To Reproduce**\r\n\r\nCreate multiple OS types , some which dont support Egressing isnt supported on Windows, thus, the port 10351 isnt opened up.\r\n\r\n**Expected**\r\nantrea agent would be more cautious about what nodes it tried to cluster with.\r\n\r\n**Actual behavior**\r\n\r\nsome scary log messages about failures occur which can make it difficult to debug other networking issues that might be going on in mixed OS clusters, which are common.\r\n "
    },
    {
     "from": "gpt",
     "value": "['transit']"
    }
   ]
  },
  {
   "id": "identity_12",
   "conversations": [
    {
     "from": "human",
     "value": "title: Flow matching for DNS responses is invalid. body: **Describe the bug**\r\nThese are some of the OVS conjunctive flows to match DNS response packets:\r\n```\r\n cookie=0x9020000000000, duration=49.585s, table=AntreaPolicyIngressRule, n_packets=0, n_bytes=0, priority=64991,conj_id=1 actions=controller(reason=no_match,max_len=128,id=28130,userdata=02,pause),resubmit(,IngressMetric)\r\n cookie=0x9020000000000, duration=39.274s, table=AntreaPolicyIngressRule, n_packets=0, n_bytes=0, priority=64991,udp,tp_src=53 actions=conjunction(1,1/2)\r\n cookie=0x9020000000000, duration=39.274s, table=AntreaPolicyIngressRule, n_packets=0, n_bytes=0, priority=64991,tcp,tp_src=53,tcp_flags=+psh+ack actions=conjunction(1,1/2)\r\n```\r\n\r\nFor DNS packets over TCP, the flow matches on the ACK and PSH (PUSH) TCP flags.\r\n\r\nHowever, for multi-packet responses, the  PSH flag will only be set for the last packet. This is the signal to tell the receiver's TCP stack that \"all the packets in the TCP message have been received, do not wait and pass the data on to the application\". The PSH flag will typically be set by the sender's TCP stack.\r\n\r\nWith the current flows, this means that only the last packet in the DNS message will be sent for processing to the Antrea Agent. This is a definite issue as the Antrea Agent is meant to process the _first_ packet in the message:\r\n\r\nhttps://github.com/antrea-io/antrea/blob/6363bdbfc55d19789c05cf4a5e13ec96e18e1a0d/pkg/agent/controller/networkpolicy/fqdn.go#L743-L770\r\n\r\nTo validate my statement above, I created a \"multi-string\" TXT record in AWS Route 53. The record name is `dnstest.abas.link`. It consists of 6 strings, each of length 250 chars (255 is the max). 6*250 is 1,500, and with all the overhead the full message size is around 1,600B, which means that the message exceeds the MSS and has to be split over 2 TCP packets.\r\n\r\nOne can then run `dig dnstest.abas.link @8.8.8.8 TXT +tcp` to get the DNS response. When capturing the packets, we get:\r\n```\r\n18:26:37.525505 IP 192.168.50.189.64684 > dns.google.domain: Flags [SEW], seq 777548318, win 65535, options [mss 1460,nop,wscale 6,nop,nop,TS val 2077381621 ecr 0,sackOK,eol], length 0\r\n18:26:37.530449 IP dns.google.domain > 192.168.50.189.64684: Flags [S.], seq 702672513, ack 777548319, win 65535, options [mss 1412,sackOK,TS val 1867459792 ecr 2077381621,nop,wscale 8], length 0\r\n18:26:37.530574 IP 192.168.50.189.64684 > dns.google.domain: Flags [.], ack 1, win 2056, options [nop,nop,TS val 2077381626 ecr 1867459792], length 0\r\n18:26:37.530756 IP 192.168.50.189.64684 > dns.google.domain: Flags [P.], seq 1:49, ack 1, win 2056, options [nop,nop,TS val 2077381626 ecr 1867459792], length 48 27694+ [1au] TXT? dnstest.abas.link. (46)\r\n18:26:37.534885 IP dns.google.domain > 192.168.50.189.64684: Flags [.], ack 49, win 256, options [nop,nop,TS val 1867459797 ecr 2077381626], length 0\r\n18:26:37.542951 IP dns.google.domain > 192.168.50.189.64684: Flags [.], seq 1:1401, ack 49, win 256, options [nop,nop,TS val 1867459805 ecr 2077381626], length 1400 [prefix length(1624) != length(1398)] (invalid)\r\n18:26:37.542953 IP dns.google.domain > 192.168.50.189.64684: Flags [P.], seq 1401:1627, ack 49, win 256, options [nop,nop,TS val 1867459805 ecr 2077381626], length 226 [prefix length(26214) != length(224)] (invalid)\r\n18:26:37.543082 IP 192.168.50.189.64684 > dns.google.domain: Flags [.], ack 1627, win 2030, options [nop,nop,TS val 2077381639 ecr 1867459805], length 0\r\n18:26:37.544803 IP 192.168.50.189.64684 > dns.google.domain: Flags [F.], seq 49, ack 1627, win 2048, options [nop,nop,TS val 2077381640 ecr 1867459805], length 0\r\n18:26:37.549485 IP dns.google.domain > 192.168.50.189.64684: Flags [F.], seq 1627, ack 50, win 256, options [nop,nop,TS val 1867459811 ecr 2077381640], length 0\r\n18:26:37.549611 IP 192.168.50.189.64684 > dns.google.domain: Flags [.], ack 1628, win 2048, options [nop,nop,TS val 2077381645 ecr 1867459811], length 0\r\n``` \r\n\r\nIn particular, these 2 packets constitute the DNS response:\r\n```\r\n18:26:37.542951 IP dns.google.domain > 192.168.50.189.64684: Flags [.], seq 1:1401, ack 49, win 256, options [nop,nop,TS val 1867459805 ecr 2077381626], length 1400 [prefix length(1624) != length(1398)] (invalid)\r\n18:26:37.542953 IP dns.google.domain > 192.168.50.189.64684: Flags [P.], seq 1401:1627, ack 49, win 256, options [nop,nop,TS val 1867459805 ecr 2077381626], length 226 [prefix length(26214) != length(224)] (invalid)\r\n```\r\n\r\nNotice that only the second one has the PSH flag (`P`) set.\r\n\r\n**Versions:**\r\nAntrea v1.13 and ToT\r\n"
    },
    {
     "from": "gpt",
     "value": "['network-policy']"
    }
   ]
  }
]